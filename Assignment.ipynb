{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9qnjbu82ZEtIIWkIvC9hf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-H-GkpqVKf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a21bf39c-9407-414a-b479-940d3178e8f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpW-BRmepcPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f960a4a5-bf4f-4be7-80b9-6156e8ed2583"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Retail Pulse ML Assignment Data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Retail Pulse ML Assignment Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG5TP5cWp7zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "from numpy import savez_compressed\n",
        "from numpy import load\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,UpSampling2D\n",
        "from keras.layers import Input\n",
        "from keras.applications import VGG19\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import InputLayer, BatchNormalization\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.applications import InceptionV3\n",
        "# from tensorflow.keras.applications import Xception # TensorFlow ONLY\n",
        "# from tensorflow.keras.applications import VGG16\n",
        "# from tensorflow.keras.applications import VGG19\n",
        "# from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vel7c6OnrBTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id = []\n",
        "family = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSjzSHerJp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('images_family_train.txt','r')\n",
        "for line in f:\n",
        "  idd = line[:7]\n",
        "  fam = line[7:]\n",
        "  fam = fam.strip(\"\\n\")\n",
        "  fam = fam.strip(\" \")\n",
        "  family.append(fam)\n",
        "  id.append(idd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTKkBwo9JrAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "variants = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT9AKuJN6p8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = open(\"images_variant_train.txt\",\"r\")\n",
        "for line in g:\n",
        "  var = line[7:]\n",
        "  var = var.strip(\"\\n\")\n",
        "  var = var.strip(\" \")\n",
        "  variants.append(var)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We6RdTjZJ11I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame()\n",
        "# data['Id'] = id\n",
        "data['Variants'] = variants\n",
        "data['Family'] = family"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5kDszFQK8hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = data.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQdhV_pBF6KT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "95d38aa2-29d2-4909-e075-4337c3281139"
      },
      "source": [
        "new_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variants</th>\n",
              "      <th>Family</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>707-320</td>\n",
              "      <td>Boeing 707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>727-200</td>\n",
              "      <td>Boeing 727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>737-200</td>\n",
              "      <td>Boeing 737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>737-300</td>\n",
              "      <td>Boeing 737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>737-400</td>\n",
              "      <td>Boeing 737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Spitfire</td>\n",
              "      <td>Spitfire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Tornado</td>\n",
              "      <td>Tornado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Tu-134</td>\n",
              "      <td>Tu-134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Tu-154</td>\n",
              "      <td>Tu-154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Yak-42</td>\n",
              "      <td>Yak-42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Variants      Family\n",
              "0    707-320  Boeing 707\n",
              "1    727-200  Boeing 727\n",
              "2    737-200  Boeing 737\n",
              "3    737-300  Boeing 737\n",
              "4    737-400  Boeing 737\n",
              "..       ...         ...\n",
              "95  Spitfire    Spitfire\n",
              "96   Tornado     Tornado\n",
              "97    Tu-134      Tu-134\n",
              "98    Tu-154      Tu-154\n",
              "99    Yak-42      Yak-42\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC0dt0PVsESR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_with_id = pd.DataFrame()\n",
        "data_with_id['Id'] = id\n",
        "data_with_id['Variants'] = variants\n",
        "data_with_id['Family'] = family"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwI68wUWsSBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ytrain = pd.DataFrame()\n",
        "ytrain['family'] = family"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la8k3-0etxZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate binary values using get_dummies\n",
        "fam_df = pd.get_dummies(ytrain, columns=[\"family\"], prefix=[\"Type_is\"] )\n",
        "# merge with main df bridge_df on key values\n",
        "ytrain = ytrain.join(fam_df)\n",
        "ytrain.drop(\"family\",axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7OMA7WxT6Ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_family = []\n",
        "f = open('images_family_val.txt','r')\n",
        "for line in f:\n",
        "  var = line[7:]\n",
        "  var = var.strip(\"\\n\")\n",
        "  var = var.strip(\" \")\n",
        "  val_family.append(var)\n",
        "ytest = pd.DataFrame()\n",
        "ytest['val_family'] = val_family\n",
        "variants_val_df = pd.get_dummies(ytest, columns=[\"val_family\"], prefix=[\"Type_is\"] )\n",
        "ytest = ytest.join(variants_val_df)\n",
        "ytest.drop(\"val_family\",axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-5PC-qNt48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = \"/content/drive/My Drive/Retail Pulse ML Assignment Data/train\"\n",
        "val_path = \"/content/drive/My Drive/Retail Pulse ML Assignment Data/val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzK-qLTs7lQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = []\n",
        "i = 0\n",
        "for f in sorted(listdir(train_path)):\n",
        "  all_imgs_in_folder = []\n",
        "  for g in sorted(listdir(join(train_path,f))):\n",
        "    img = load_img(join(join(train_path,f),g),target_size=(32,32))\n",
        "    img = np.array(img, dtype=np.float16) / 255.0\n",
        "    all_imgs_in_folder.append(img)\n",
        "    print(i)\n",
        "    i+=1\n",
        "  training_set.extend(all_imgs_in_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1_jY3FNv-Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "savez_compressed(\"training_set_array_32.npz\",training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kG6fTEvdpK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_data = load('training_set_array_32.npz')\n",
        "training_set = dict_data['arr_0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVYA9PwaIb9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training_set = training_set.reshape(-1,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzM3_Oc6F3GC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = training_set.reshape(-1,32,32,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhiDeZHMUIjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_set = []\n",
        "i = 0\n",
        "for f in sorted(listdir(val_path)):\n",
        "  all_imgs_in_folder = []\n",
        "  for g in sorted(listdir(join(val_path,f))):\n",
        "    img = load_img(join(join(val_path,f),g),target_size=(32,32))\n",
        "    img = np.array(img, dtype=np.float16) / 255.0\n",
        "    all_imgs_in_folder.append(img)\n",
        "    print(i)\n",
        "    i+=1\n",
        "  val_set.extend(all_imgs_in_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7CXQ7K5wTSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "savez_compressed(\"val_set_array_32.npz\",val_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hInE2IDedawm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_data = load('val_set_array_32.npz')\n",
        "val_set = dict_data['arr_0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tak7l0XIjcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_set = val_set.reshape(-1,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYD7M2sNdLG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_set = val_set.reshape(-1,32,32,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1KqVjbH7HB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rotation_range=12,\n",
        "                             width_shift_range= 1.5,\n",
        "                             height_shift_range = 0.4,\n",
        "                             horizontal_flip = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRx9GaG8Fvn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen.fit(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J61c2JQCoR7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32,32,3),kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(70, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6P8j7TXnMg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "9bc70e7d-429f-483a-a93e-fd1f90655286"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_27 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 13, 13, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 4, 4, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 4, 4, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 2, 2, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 2, 2, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 70)                9030      \n",
            "=================================================================\n",
            "Total params: 180,262\n",
            "Trainable params: 178,726\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpbNm0cyXnG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"/content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keajeva_GYgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b98ccc10-1836-4bdb-adaa-6781f7e73c13"
      },
      "source": [
        "history = model.fit(datagen.flow(training_set, ytrain, batch_size=32),\n",
        "          steps_per_epoch=(3*len(training_set) / 32), epochs=100,validation_data=(val_set,ytest),callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 4.2153 - accuracy: 0.0712 - val_loss: 4.1058 - val_accuracy: 0.0813\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.08131, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 2/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 4.1420 - accuracy: 0.0740 - val_loss: 4.0326 - val_accuracy: 0.0849\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.08131 to 0.08491, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 3/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 4.0558 - accuracy: 0.0780 - val_loss: 4.6757 - val_accuracy: 0.0792\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.08491\n",
            "Epoch 4/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 4.0079 - accuracy: 0.0845 - val_loss: 3.9563 - val_accuracy: 0.0972\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.08491 to 0.09721, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 5/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.9448 - accuracy: 0.0874 - val_loss: 3.9124 - val_accuracy: 0.0945\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.09721\n",
            "Epoch 6/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.9179 - accuracy: 0.0893 - val_loss: 3.9452 - val_accuracy: 0.0987\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.09721 to 0.09871, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 7/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.8412 - accuracy: 0.0984 - val_loss: 4.0742 - val_accuracy: 0.0816\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.09871\n",
            "Epoch 8/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.8167 - accuracy: 0.0991 - val_loss: 3.8095 - val_accuracy: 0.1029\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.09871 to 0.10291, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 9/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.7642 - accuracy: 0.1091 - val_loss: 3.9149 - val_accuracy: 0.0969\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.10291\n",
            "Epoch 10/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.7216 - accuracy: 0.1086 - val_loss: 3.6598 - val_accuracy: 0.1269\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.10291 to 0.12691, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 11/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.6787 - accuracy: 0.1183 - val_loss: 3.7993 - val_accuracy: 0.1131\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.12691\n",
            "Epoch 12/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.6459 - accuracy: 0.1259 - val_loss: 3.7736 - val_accuracy: 0.1083\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.12691\n",
            "Epoch 13/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.6260 - accuracy: 0.1276 - val_loss: 3.8655 - val_accuracy: 0.1182\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.12691\n",
            "Epoch 14/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.5891 - accuracy: 0.1350 - val_loss: 3.8748 - val_accuracy: 0.1125\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.12691\n",
            "Epoch 15/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.5562 - accuracy: 0.1459 - val_loss: 3.6736 - val_accuracy: 0.1413\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.12691 to 0.14131, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 16/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.5386 - accuracy: 0.1459 - val_loss: 3.9398 - val_accuracy: 0.1137\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.14131\n",
            "Epoch 17/100\n",
            "313/312 [==============================] - 10s 33ms/step - loss: 3.4982 - accuracy: 0.1553 - val_loss: 3.6748 - val_accuracy: 0.1440\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.14131 to 0.14401, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 18/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.4649 - accuracy: 0.1584 - val_loss: 3.4457 - val_accuracy: 0.1608\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.14401 to 0.16082, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 19/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.4394 - accuracy: 0.1652 - val_loss: 3.6269 - val_accuracy: 0.1368\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.16082\n",
            "Epoch 20/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.4218 - accuracy: 0.1688 - val_loss: 3.5221 - val_accuracy: 0.1731\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.16082 to 0.17312, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 21/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.4076 - accuracy: 0.1771 - val_loss: 3.4285 - val_accuracy: 0.1707\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.17312\n",
            "Epoch 22/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3612 - accuracy: 0.1853 - val_loss: 3.5562 - val_accuracy: 0.1710\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.17312\n",
            "Epoch 23/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3711 - accuracy: 0.1806 - val_loss: 3.8954 - val_accuracy: 0.1428\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.17312\n",
            "Epoch 24/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3507 - accuracy: 0.1836 - val_loss: 3.5063 - val_accuracy: 0.1692\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.17312\n",
            "Epoch 25/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3225 - accuracy: 0.1892 - val_loss: 3.5423 - val_accuracy: 0.1758\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.17312 to 0.17582, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 26/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3097 - accuracy: 0.1901 - val_loss: 3.4770 - val_accuracy: 0.1884\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.17582 to 0.18842, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 27/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3019 - accuracy: 0.1931 - val_loss: 3.8901 - val_accuracy: 0.1389\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.18842\n",
            "Epoch 28/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.3082 - accuracy: 0.1926 - val_loss: 3.3680 - val_accuracy: 0.1872\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.18842\n",
            "Epoch 29/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2819 - accuracy: 0.1969 - val_loss: 3.4851 - val_accuracy: 0.1884\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.18842\n",
            "Epoch 30/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2906 - accuracy: 0.1972 - val_loss: 3.7471 - val_accuracy: 0.1806\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.18842\n",
            "Epoch 31/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2520 - accuracy: 0.2050 - val_loss: 3.3230 - val_accuracy: 0.2073\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.18842 to 0.20732, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 32/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2507 - accuracy: 0.1983 - val_loss: 3.4277 - val_accuracy: 0.2016\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.20732\n",
            "Epoch 33/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2432 - accuracy: 0.2020 - val_loss: 3.3750 - val_accuracy: 0.1959\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.20732\n",
            "Epoch 34/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2363 - accuracy: 0.2077 - val_loss: 3.6032 - val_accuracy: 0.1758\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.20732\n",
            "Epoch 35/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2409 - accuracy: 0.2054 - val_loss: 3.4200 - val_accuracy: 0.1950\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.20732\n",
            "Epoch 36/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.2021 - accuracy: 0.2110 - val_loss: 3.3556 - val_accuracy: 0.2076\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.20732 to 0.20762, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 37/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1730 - accuracy: 0.2191 - val_loss: 3.3276 - val_accuracy: 0.2142\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.20762 to 0.21422, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 38/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1642 - accuracy: 0.2236 - val_loss: 3.4611 - val_accuracy: 0.1911\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.21692\n",
            "Epoch 42/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1550 - accuracy: 0.2250 - val_loss: 3.5727 - val_accuracy: 0.1923\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.21692\n",
            "Epoch 43/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1278 - accuracy: 0.2396 - val_loss: 3.5576 - val_accuracy: 0.1650\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.21692\n",
            "Epoch 44/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1462 - accuracy: 0.2300 - val_loss: 3.4234 - val_accuracy: 0.2193\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.21692 to 0.21932, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 45/100\n",
            "313/312 [==============================] - 10s 33ms/step - loss: 3.1327 - accuracy: 0.2332 - val_loss: 3.3250 - val_accuracy: 0.2109\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.21932\n",
            "Epoch 46/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1359 - accuracy: 0.2325 - val_loss: 3.4403 - val_accuracy: 0.2175\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.21932\n",
            "Epoch 47/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1318 - accuracy: 0.2290 - val_loss: 3.3699 - val_accuracy: 0.2187\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.21932\n",
            "Epoch 48/100\n",
            "313/312 [==============================] - 10s 33ms/step - loss: 3.1200 - accuracy: 0.2323 - val_loss: 3.6171 - val_accuracy: 0.1917\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.21932\n",
            "Epoch 49/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1161 - accuracy: 0.2373 - val_loss: 3.4097 - val_accuracy: 0.2160\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.21932\n",
            "Epoch 50/100\n",
            "313/312 [==============================] - 10s 33ms/step - loss: 3.1056 - accuracy: 0.2394 - val_loss: 3.3185 - val_accuracy: 0.2259\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.21932 to 0.22592, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 51/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.1013 - accuracy: 0.2394 - val_loss: 3.2911 - val_accuracy: 0.2247\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.22592\n",
            "Epoch 52/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0738 - accuracy: 0.2419 - val_loss: 3.4749 - val_accuracy: 0.2049\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.22592\n",
            "Epoch 53/100\n",
            "313/312 [==============================] - 10s 33ms/step - loss: 3.0855 - accuracy: 0.2343 - val_loss: 3.3202 - val_accuracy: 0.2370\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.22592 to 0.23702, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 54/100\n",
            "313/312 [==============================] - 11s 35ms/step - loss: 3.0814 - accuracy: 0.2425 - val_loss: 3.2784 - val_accuracy: 0.2352\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.23702\n",
            "Epoch 55/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0802 - accuracy: 0.2385 - val_loss: 3.3014 - val_accuracy: 0.2286\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.23702\n",
            "Epoch 56/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0737 - accuracy: 0.2432 - val_loss: 3.4519 - val_accuracy: 0.2130\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.23702\n",
            "Epoch 57/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0563 - accuracy: 0.2492 - val_loss: 3.9055 - val_accuracy: 0.1893\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.23702\n",
            "Epoch 58/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0591 - accuracy: 0.2526 - val_loss: 3.3523 - val_accuracy: 0.2229\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.23702\n",
            "Epoch 59/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0479 - accuracy: 0.2449 - val_loss: 3.4915 - val_accuracy: 0.2115\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.23702\n",
            "Epoch 60/100\n",
            "313/312 [==============================] - 10s 33ms/step - loss: 3.0415 - accuracy: 0.2500 - val_loss: 3.5442 - val_accuracy: 0.1980\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.23702\n",
            "Epoch 61/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0398 - accuracy: 0.2562 - val_loss: 3.4572 - val_accuracy: 0.2043\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.23702\n",
            "Epoch 62/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0414 - accuracy: 0.2502 - val_loss: 3.2383 - val_accuracy: 0.2379\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.23702 to 0.23792, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 63/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0324 - accuracy: 0.2547 - val_loss: 3.3167 - val_accuracy: 0.2271\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.23792\n",
            "Epoch 64/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0215 - accuracy: 0.2544 - val_loss: 3.3533 - val_accuracy: 0.2316\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.23792\n",
            "Epoch 65/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0158 - accuracy: 0.2587 - val_loss: 3.2751 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00065: val_accuracy improved from 0.23792 to 0.24002, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 66/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0238 - accuracy: 0.2550 - val_loss: 3.4825 - val_accuracy: 0.2211\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.24002\n",
            "Epoch 67/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0080 - accuracy: 0.2696 - val_loss: 3.3582 - val_accuracy: 0.2307\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.24002\n",
            "Epoch 68/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0059 - accuracy: 0.2583 - val_loss: 3.3776 - val_accuracy: 0.2361\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.24002\n",
            "Epoch 69/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0008 - accuracy: 0.2595 - val_loss: 3.2662 - val_accuracy: 0.2514\n",
            "\n",
            "Epoch 00069: val_accuracy improved from 0.24002 to 0.25143, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 70/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 3.0166 - accuracy: 0.2594 - val_loss: 3.2661 - val_accuracy: 0.2535\n",
            "\n",
            "Epoch 00070: val_accuracy improved from 0.25143 to 0.25353, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 71/100\n",
            "313/312 [==============================] - 11s 35ms/step - loss: 2.9995 - accuracy: 0.2591 - val_loss: 3.1363 - val_accuracy: 0.2682\n",
            "\n",
            "Epoch 00071: val_accuracy improved from 0.25353 to 0.26823, saving model to /content/drive/My Drive/Retail Pulse ML Assignment Data/weights_best.hdf5\n",
            "Epoch 72/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 2.9979 - accuracy: 0.2555 - val_loss: 3.3419 - val_accuracy: 0.2379\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.26823\n",
            "Epoch 73/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 2.9944 - accuracy: 0.2643 - val_loss: 3.4690 - val_accuracy: 0.2112\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.26823\n",
            "Epoch 74/100\n",
            "313/312 [==============================] - 11s 34ms/step - loss: 2.9909 - accuracy: 0.2641 - val_loss: 3.3149 - val_accuracy: 0.2355\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.26823\n",
            "Epoch 75/100\n",
            "205/312 [==================>...........] - ETA: 3s - loss: 2.9781 - accuracy: 0.2692"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-bc916e02ae6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(datagen.flow(training_set, ytrain, batch_size=32),\n\u001b[0;32m----> 2\u001b[0;31m           steps_per_epoch=(3*len(training_set) / 32), epochs=100,validation_data=(val_set,ytest),callbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m                 initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MKYLbdtG4Jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56b48249-41f1-4761-9a35-deb6ca4b955a"
      },
      "source": [
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#     directory=train_path,\n",
        "#     target_size=(28, 28),\n",
        "#     color_mode=\"grayscale\",\n",
        "#     batch_size=32,\n",
        "#     class_mode=\"categorical\",\n",
        "#     shuffle=False\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3334 images belonging to 100 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WuCoO1dko8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "#                                    rotation_range=40,\n",
        "#                                    width_shift_range=0.2,\n",
        "#                                    height_shift_range=0.2,\n",
        "#                                    shear_range=0.2,\n",
        "#                                    zoom_range=0.2,\n",
        "#                                    horizontal_flip=True,\n",
        "#                                    fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAB5L3WbJq-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPZPVZA7Ib16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e0a1856-1c22-4a38-d235-bd327849dabc"
      },
      "source": [
        "# valid_generator= val_datagen.flow_from_directory(\n",
        "#     directory=val_path,\n",
        "#     target_size = (28,28),\n",
        "#     color_mode = \"grayscale\",\n",
        "#     batch_size =32,\n",
        "#     class_mode=\"categorical\"\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3333 images belonging to 100 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgPWC0LLLJ07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn4.fit_generator(\n",
        "#         train_generator,\n",
        "#         steps_per_epoch=3334 // 32,\n",
        "#         epochs=10\n",
        "#         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D24glTKn1wf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base_model = ResNet50(weights= \"imagenet\", include_top=False, input_shape= (32,32,3))\n",
        "# for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(400, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# # softmax classifier\n",
        "# model.add(Dense(100,activation='softmax'))\n",
        "\n",
        "# pretrainedInput = base_model.input\n",
        "# pretrainedOutput = base_model.output\n",
        "# output = model(pretrainedOutput)\n",
        "# model = Model(pretrainedInput, output)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrgFW4aL13E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = base_model.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dropout(0.7)(x)\n",
        "# predictions = Dense(100, activation= 'softmax')(x)\n",
        "# model = Model(inputs = base_model.input, outputs = predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erFU-tEhFVl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVFXFX5yG-pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape = (28, 28, 1))\n",
        "def encoder(input_img):\n",
        "    #encoder\n",
        "    #input = 28 x 28 x 1 (wide and thin)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    return conv4\n",
        "\n",
        "def decoder(conv4):    \n",
        "    #decoder\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
        "    return decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHLrMQkzHC3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(input_img, decoder(encoder(input_img)))\n",
        "model.compile(loss='mean_squared_error', optimizer = keras.optimizers.RMSprop())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}